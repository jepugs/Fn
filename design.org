#+TITLE: fn Language Reference (WIP)

* To-Do

- use cmake
- add unit tests for scanner, compiler, interpreter
- let compiler/local variables
- function compiler
- begin work on standard library
- write source location finder for bytecode
- add proper exception generation to the compiler
- add a toplevel exception handler
- think about ffi
- add ns global variable
- create ast intermediate data structure


* Version 0 Feature List

Data types: Num, String, Symbol, True, False, Null, Function, List, and Object.

Functions can accept.

For now, modules and files are in 1-to-1 correspondence. import can accept either a string (a
pathname relative to the source file) or a symbol (a pathname relative to the source file, but
without the ".fn").

Special operators:
- Control flow operators :: and, cond, do, if, or
- Definitions and Variables :: def, get, let, set
- Other :: apply, import, fn

No macros yet.

** Standard library functions:
*Tests*
- =, is
- list?, empty?, bool?, null?, symbol?, object?, function?
- not

*Misc*
- +,-,*,/,sqrt,^,log
- show, print, read, eval
- symbol-name
- type-of

*Objects*
- keys

*Lists*
- head, tail
- append, reverse, length, nth, subseq (all also work on strings)
- cons, list, range
- map, filter, reduce
- as-list (works on strings and objects)

*Strings*
- string (makes an argument list of values into a string)


** Next features to add

After version 0, the next things to add will be
- structural pattern matching with case
- more sophisticated module/name resolution behavior
- dynamic variables
- quasiquote and macros (i.e. quasiquote, defmacro, in-user-environment)
- maybe arrays
- lazy sequences
- class/constructor/inheritance stuff built atop the object system


* Object System

The object system of fn is similar to that of Lua or Javascript. Objects do not have a type
associated with them, although a class slot may be added to an object (it will probably be
identified with the symbol ~'_class~, ~'Class~, or simply ~'class~).

We borrow javascript terminology for objects. An object contains 0 or more *values*, each of which is
indexed by a unique *key*. A key-value pair is referred to as a *property*.

Values may be accessed using two forms: ~dot~ and ~get~:

#+BEGIN_SRC fn
;; note: a.b is internally treated as (dot a b)
key.value
(get key 'value)
#+END_SRC

The main difference is that ~get~ evaluates its second argument, while ~dot~ does not.

Keys may only be symbols for now. Later on, I'd like to copy what Lua does and allow general objects
to be keys, since this opens up a big world of possibilities, but because of ambiguity in what it
means to index an object with an object, this will have to wait until I make some more design
decisions.

Object keys may be set with the ~set~ operator, which works like this:

#+BEGIN_SRC fn
(set obj.key new-value)
;; or
(set (get obj 'key) new-value)
#+END_SRC


* Modules

Modules are fn's facility for multi-file projects and library organization. The best way to
illustrate how modules work is by example:

#+BEGIN_SRC fn
;; source file: my-mod.fn
(def (my-fun x)
  (* x x))

;; source file: main.fn
(import my-mod)
(println (my-mod.my-fun 4))
;; prints 16
#+END_SRC

As can be seen in the above example, each module corresponds to a source file. The module above is
automatically named "my-mod" based upon its filename. Module names can also be explicitly specified
by setting the global property ~mod-info.name~ from within the module. The ~import~ operator is used to
import other modules. In the above, it creates a global variable called ~my-mod~ to hold the imported
module. Imported modules are just normal objects, with metadata contained in the ~mod-info~ property.


** import syntax and search paths

The import operator accepts a symbol or a dot form as an argument. Dots in the symbol indicate
descent into a directory, so for instance ~(import a.b)~ will look for files names "a/b.fn" to import.

(As a result, there is currently no way to import files which reside in directories that have dots
in their names or which have multiple dots in their names. This will likely be resolved via changes
to how dot is parsed).

Currently the only search path is the directory in which the file currently resides. In the future
there will be a second one in ${HOME}/.local/lib/fn/ and a third in ${PREFIX}/lib/fn/.

** Setting import names

The import command supports an ~as~ directive which offers control over the name of the created
variable:

#+BEGIN_SRC fn
(import my-mod as m)
;; creates a global object named m
m.my-fun
#+END_SRC

** Packages and short names

To avoid name collisions, it is recommended that all library code use the naming convention
"package.module-name" or "package.subpackages...module-name". For example,

#+BEGIN_SRC fn
;; source file: demo-library/utility.fn
(set mod-info.name "demo-library.utility")
#+END_SRC

In this case, the string after the last dot (module-name) will be used as the *short name* for the
module, while the string before that is termed the *package*. When import is used, it uses the short
name unless an explicit alternative is specified using ~as~.

** ~ns~ and Unique Global Identifiers

~ns~ is a special global object. Whenever a module is imported, it is added to ~ns~ via its full name.
~ns~ has a tree structure, so for instance a module named ~pack.sub.mod~ can be accessed anywhere in the
program as ~ns.pack.sub.mod~. This is as opposed to modules which have been locally imported, which
will have a global variable corresponding to them.

This behavior is exploited by fn's macro system.


* Macros

fn's macro system is very similar to that of Common Lisp or Clojure. fn is truly homoiconic and its
macro system is entirely unhygienic, because hygiene is unnecessary and occasionally very annoying.

To understand fn's macros, first you must understand that all fn source code can be represented via
fn's native data structures, namely lists and literals (i.e. symbols, numbers, and strings). A macro
is a function whose arguments and output are code in this representation. At compile time the macro
is expanded-- that is, the macro function is evaluated and the place it occupied is replaced with
the newly-generated code.

** Quasiquote

The backtick ` character is used to denote quasiquote syntax. Quasiquote syntax works like quote
until it encounters an unquote form, which is denoted by the characters , or ,@. It then evaluates
the expression following , or ,@ and inserts it into the resultant form.

#+BEGIN_SRC fn
`(test qq ,(+ 2 2))
;; gives a list ['test 'qq 4]

`(test qq ,@(map $(+ 3 $) [1 2 3]))
;; gives a list ['test 'qq 4 5 6]
#+END_SRC

There is another difference between quote and quasiquote. When quasiquote encounters a dot form, it
will automatically expand it into a unique global variable accessor (e.g. ns.module-name.var-name)
if possible. This allows access of global variables from macroexpanded code, for instance:

#+BEGIN_SRC fn
`fn.map
;; expands to 'ns.fn.map, or (dot ns fn map)

;; this macro expands to use the built-in functions fn.map and fn.range
(defmacro map-times (var-name times & function-body)
  `(fn.map ,var-name 
     (fn ,var-name ,@function-body)
     (fn.range ,times)))
#+END_SRC

** ~gensym~ and ~`#~ syntax

Sometimes, a macro needs to expand into code which creates a temporary variable. For this purpose,
it is crucial that the macro be able to bind variables without creating a name collision with any of
the user variables. The function ~gensym~ creates a unique symbol which can be used for this purpose.
On each call, ~gensym~ is guaranteed to create a brand new symbol which has never occurred in code
before (and cannot occur unintentionally again).

For example:

#+BEGIN_SRC fn
(defmacro macro-swap (a b)
  (let (tmp-var (gensym))
    `(let (,tmp-var ,a)
       (set ,a ,b)
       (set ,b ,tmp-var))))
#+END_SRC

Declaring gensyms in this manner can be tedious, so we also add special syntax to the quasiquote
operator. Namely, symbols whose names begin with ~#~ are automatically replaced with gensyms. Thus,
the above could have been written like so:

#+BEGIN_SRC fn
(defmacro macro-swap (a b)
  `(let (#tmp ,a)
     (set ,a ,b)
     (set ,b #tmp)))
#+END_SRC



* Implementation details

** Data Types

 Version 0 data types:
 - Num :: 64-bit floating point number (could decrease bits later to 61 if I want to fit the
          value+tag both into 64 bits)
 - String :: UTF-8 string (there are no individual characters in fn, just strings. Characters can be
             represented by integers if ya really need).
 - Symbol :: an internalized string. Also used to represent variable names.
 - True, False, and Null :: Boolean true/false and a null value. null should probably act as false.
      Yes, it's null not nil
 - List :: singley-linked list. Cons cell. The empty list is a special value, NOT the same as null.
           Dotted pairs are not allowed.
 - Object :: key-value store. For now, use a hash table and only allow symbols as keys. In the
             future, a hybrid hash table/array approach (as used by Lua) may be better. In fact, a
             trie might even be better.
 - Function :: what it sounds like. Internally this is probably just gonna be a bytecode address

 Probable future data types:
 - Integer :: dedicated integer type w/ unbounded range
 - Array/Vector :: fixed-size array (constant-time access). Might not be necessary if we use the
                   hybrid data structure Lua uses for tables
 - Bytes, ForeignStructs, etc :: bit-fiddley ways to engage with foreign functions and system-level
      stuff

*** String Encoding (UTF-8)

 Here's the deal. UTF-8 is the right encoding and we will use it. This creates some issues when
 comparing text for equality, since we might have different code points representing the same
 characters. The Unicode Consortium's ordained solution to this problem is to use one of the NFD or
 NFC normalized forms, which ensure that each character has a unique representation. Normalization is
 relatively cheap, but we shouldn't go fussing about with our users' bytes without their permission.

 This raises an interesting problem. When we compare strings for equality, should we compare
 characters or code points? Naively, the "right" answer seems to be to compare characters, but if we
 suppose that (for some reason, e.g. printing text) we actually care about which codepoints go into
 each character, then we have a problem.

 The most common solution in practice seems to be to do byte-by-byte comparison. This gives an
 advantage in performance and implementation simplicity, but can lead to the kinds of localization
 problems that necessitate UTF-8 in the first place.

 Given Rob Pike's history designing UTF-8, it seems safe to copy Go. This involves writing a sensible
 set of (standard library) utilities for working with UTF-8 strings and runes, including providing
 facilities for normalization and doing normalization automatically where appropriate. For instance,
 it seems sensible to enforce that all programmatically-defined strings are in NFC format, and to
 provide a keyword argument for any text reading operations to automatically convert things to that
 format on the fly (since it should be much cheaper to normalize a string during a sequential read
 than to go back and normalize the whole thing later. Saves an entire pass over the string, which is
 important if it's long).


*** Bitwise representation

 Values in fn are exactly 64 bits. There is a variable length tag describing the value contained in
 the first 3 bits (for pointers and numbers) or the first 8 bits (for other values). This is
 convenient because it allows a single x86_64 register to be used for each value.

**** Initial tagging scheme

  *3-bit Tags*

  In order to cheat on floating point operations, the 3 LSB are used as the tag. This way, if we stick
  to 8-byte aligned pointers, we can fit an entire pointer in the next 61 bytes.

  The bytes (in binary) are:

  - Num :: 000 - 61-bit floating point number. Can use normal FP operations since least significant
    bits are 0. Must remember to mask off bits after operation.
  - List :: 001 - Raw pointer to 16-byte cons data structure.
  - String :: 010 - Raw pointer to string data structure. First 4 bytes of string are index.
  - Object :: 011 - Raw pointer to object (aka hash table) data structure
  - Function :: 100 - Raw pointer to function data structure
  - Extended tag :: 111 - Indicates that an 8-bit tag is used


  *8-bit tags*

  When the first three bits indicate the extended tag, the next 5 bits indicate the type.

  Only the 5 extended bits are given

  - Null :: 00000 - null value. Other bits should be 0.
  - Empty List :: 0001 - empty list. Other bits should be 0.
  - False :: 00010 - boolean false value. Other bits should be 0.
  - True :: 00011 - boolean true value. Other bits should be 0. 
  - Symbol :: 00100 - 32-bit symbol ID (aka FNV-1a hash of the string)


**** Future tagging schemes

  I hope we don't need to make a new taggin scheme. There's still space for two more pointer data
  types, and I think that bitvectors and arrays would be good candidates for those.

  If we need more tags, we can cross that bridge when we come to it, and we can get quite creative. We
  certainly don't need all 61 bits. The next step, if we desparately need more tags, is to make a pool
  of objects which has e.g. a 48-byte index.

  For 32-bit systems, if that ever matters, we could probably just use the same scheme and have
  smaller pointers.




* Additional notes

** VM Assumptions/Security

Several of the instructions for the VM make assumptions when intepreted.

- const and closure assume that the ID they're given identify entries in the Bytecode object's
  constant or function table respectively
- jump and cjump assume that their operands will result in jumps to valid memory locations
- when closure is invoked, it opens upvalues specified by the function prototype. The closure
  instruction assumes that the necessary stack and upvalues are available, but maybe it shouldn't...



** Compiler Macros

Compiler macros are a planned feature which should allow really high performance code to be written
by moving a lot of computations to compile time. I don't have any concrete ideas for how to define
them yet, and it would be premature at the phase of writing to introduce syntax or anything else
like that.

The essential feature of the compiler macros will be access to a robust type-inference system which
allows the compiler macro to ask the compiler what the expected return value of an arbitrary
expression is. We will also need to expose more information about syntax objects. Some of this
information would likely be useful to normal macros as well, so perhaps ~defmacro~ should be extended
in a way so that more of the information available at macroexpansion is exposed to the user code.
The dream is to give users a way to generate meaningful error messages during macroexpansion,
complete with line-by-line syntactic feedback, if they so desire. At minimum, this involves tagging
every form with its position in a file/input stream and the namespace it is evaluated in.

